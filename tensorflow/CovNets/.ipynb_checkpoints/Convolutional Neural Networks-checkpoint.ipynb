{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNets \n",
    "\n",
    "!['Convolutions'](p1.png)\n",
    "\n",
    "!['Convolution operator'](convolution.gif)\n",
    "\n",
    "\n",
    "### Filters \n",
    "\n",
    "- The first step for a CNN is to break up the image into smaller pieces, aka patches. \n",
    "- CNN uses filters to split an image into smaller patches. \n",
    "- The size of these patches matches the filter size.\n",
    "\n",
    "Slide filter horizontally or vertically to focus on a different piece of the image.\n",
    "\n",
    "- The amount by which the filter slides is referred to as the **'stride'**.\n",
    "  - The stride is a hyperparameter which we can tune. \n",
    "- Increasing the stride reduces the size of the model by reducing the number of total patches each layer observes.\n",
    "  - However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "- **Important idea:** *Grouping together adjacent pixels* and treating them as a collective.\n",
    "\n",
    "    - In a non-convolutional neural network, we would have ignored this adjacency. \n",
    "    - In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. I.E we would not have **taken advantage of the fact that pixels in an image are close together for a reason and have special meaning**.\n",
    "\n",
    "    - By taking advantage of local structure, CNN learns to classify local patterns, like shapes and objects, in an image.\n",
    "\n",
    "##### Filter Depth\n",
    "- It's common to have more than one filter. \n",
    "    - Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. \n",
    "    - The amount of filters in a convolutional layer is called the **filter depth**.\n",
    "    \n",
    "How many neurons does each patch connect to?\n",
    "\n",
    "- If we have a depth of `k`, we connect each patch of pixels to `k` neurons in the next layer.\n",
    "    - This gives us the height of **k** in the next layer, as shown below. \n",
    "    - In practice, **k** is a hyperparameter we tune, and most CNNs tend to pick the same starting values.\n",
    "\n",
    "Having multiple neurons for a given patch ensures that the CNN can learn to capture whatever characteristics from given data.\n",
    "- The CNN isn't \"programmed\" to look for certain characteristics. \n",
    "- Rather, it learns on its own which characteristics to notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Strides, Depth and Padding \n",
    "\n",
    "- **SAME Padding**, the output height and width are computed as:\n",
    "    - $ out\\_height =  ceil( \\frac{in\\_height} {strides[1]} ) $\n",
    "    - $ out\\_width  = ceil( \\frac{in\\_width} {strides[2]} ) $\n",
    "    \n",
    "- **VALID Padding**, No padding. Output height and width are computed as:\n",
    "    - $ out\\_height =  ceil(\\frac{in\\_height - filter\\_height + 1} {strides[1]}) $\n",
    "    - $ out\\_width  =  ceil(\\frac{in\\_width  - filter\\_width  + 1} {strides[2]}) $\n",
    "\n",
    "- **Otherwise**: $ output\\_height = \\frac{(n + 2p -f)}{s} +1  $ \n",
    "    - n: input height \n",
    "    - p: padding   \n",
    "    - f: filter height\n",
    "    - s: stride \n",
    "\n",
    "\n",
    "**Given**\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "\n",
    "# height, width, input_depth, output_depth = 8, 8, 3, 20\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "\n",
    "# batch, height, width, depth\n",
    "strides = [1, 2, 2, 1] \n",
    "padding = 'SAME'\n",
    "\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "\n",
    "- Output **shape of conv is [1, 16, 16, 20]** - A 4D to account for batch size.\n",
    "- If we switch padding from `SAME` to `VALID` then the output shape is [1, (32-8+1)/2, 13, 20]\n",
    "\n",
    "\n",
    "### Number of parameters \n",
    "\n",
    "**Given**\n",
    "- Input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "**Output Layer**\n",
    "- $ output\\_shape = \\frac{(n + 2p -f)}{s} +1  $  = 14x14x20 (HxWxD)\n",
    "\n",
    "**How many parameters does the convolutional layer have (without parameter sharing)?**\n",
    "\n",
    "- Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. \n",
    "    - Each neuron in the output layer must also connect to a single bias neuron.\n",
    "- parameters = (8 * 8 * 3 + 1) * (14 * 14 * 20) = 756,560\n",
    "    - 8 * 8 * 3 is the number of weights, plus 1 for the bias. \n",
    "    - Each weight is assigned to every single part of the output (14 * 14 * 20).\n",
    "    - why not times with  20?\n",
    "\n",
    "\n",
    "### Parameter Sharing\n",
    "\n",
    "The weights, `w`, are shared across patches for a given layer in a CNN to detect the **object or feature** regardless of where in the image the **object** is located.\n",
    "\n",
    "- This is known as *statistical invariance* or *translation invariance*\n",
    "\n",
    "The classification of a given patch in an image is determined by the weights and biases corresponding to that patch.\n",
    "- If we want a **cat** that’s in the top left patch to be classified in the same way as a **cat** in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "- This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. \n",
    "    - Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "- There’s an additional benefit to sharing parameters. \n",
    "    - If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. \n",
    "    - This does not scale well, especially for higher fidelity images. \n",
    "    - Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model.\n",
    "    \n",
    "**Given**\n",
    "- Input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "**Output Layer**\n",
    "- $ output\\_shape = \\frac{(n + 2p -f)}{s} +1  $  = 14x14x20 (HxWxD)\n",
    "\n",
    "**How many parameters does the convolutional layer have (with parameter sharing)?**\n",
    "- This is the number of parameters actually used in a convolution layer **tf.nn.conv2d()**\n",
    "- With parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel\n",
    "- So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer\n",
    "```python\n",
    "(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "Each layer in network detects more and more complex ideas.\n",
    "\n",
    "#### Layer 1: Picks out very simple shapes and patterns like lines and blobs.\n",
    "\n",
    "Example patterns that cause activations in the first layer of the network. \n",
    "- These range from simple diagonal lines (top left) to green blobs (bottom middle).\n",
    "!['Example patterns that cause activations in the first layer of the network'](layer1.png)\n",
    "\n",
    "- Each image in the above grid represents a pattern that causes the neurons in the first layer to activate\n",
    "    - They are patterns that the first layer recognizes. \n",
    "    - The top left image shows a -45 degree line, while the middle top square shows a +45 degree line.\n",
    "\n",
    "- Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.\n",
    "\n",
    "!['Example patches that activate the -45 degree line detector in the first layer'](layer2.png)\n",
    "\n",
    "#### Layer 2: Picks up more complex ideas like circles and stripes\n",
    "\n",
    "Second layer is picking up more complex ideas like circles and stripes. \n",
    "- The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.\n",
    "\n",
    "!['visualization of the second layer in the CNN'](layer_2.png)\n",
    "\n",
    "- The second layer captures complex ideas.\n",
    "- Recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).\n",
    "- The CNN learns to do this on its own. \n",
    "    - There is no special instruction for the CNN to focus on more complex objects in deeper layers.\n",
    "    - That's just how it normally works out when you feed training data into a CNN.\n",
    "\n",
    "#### Layer 3: Picks out complex combinations of features from the second layer\n",
    "\n",
    "!['visualization of the third layer in the CNN'](layer3.png)\n",
    "\n",
    "#### Layer 5\n",
    "\n",
    "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "K = 64\n",
    "iwidth = 10\n",
    "iheight = 10\n",
    "channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "fw = 5\n",
    "fh = 5\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=[None, iheight, iwidth, channels])\n",
    "weight = tf.Variable(tf.truncated_normal([fh, fw, channels, K]))\n",
    "bias   = tf.Variable(tf.zeros(K))\n",
    "\n",
    "# Strides [batch, input_height, input_width, input_channels] = [1, 2, 2, 1] \n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Max Pooling\n",
    "\n",
    "Examples of how max pooling works. \n",
    "\n",
    "!['max pooling with a 2x2 filter'](pool2.png)\n",
    "\n",
    "!['max pooling with a 2x2 filter and stride of 2'](pool1.png)\n",
    "\n",
    "- In this case, the max pooling filter has a shape of 2x2\n",
    "    - Max pooling with a 2x2 filter and stride of 2. \n",
    "- The four 2x2 colors represent each time the filter was applied to find the maximum value.\n",
    "    - As the max pooling filter slides across the input layer, the filter will output the maximum value of the 2x2 square.\n",
    "\n",
    "Conceptually, the benefit of the max pooling operation is to reduce the size of the input \n",
    "- And allow the neural network to focus on only the most important elements.\n",
    "- Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.max_pool()` function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "\n",
    "**Pooling decrease the size of the output and prevent overfitting.** Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers.\n",
    "\n",
    "For a pooling layer the output depth is the same as the input depth. Additionally, the pooling operation is applied individually for each depth slice.\n",
    "\n",
    "Recently, pooling layers have fallen out of favor. Some reasons are:\n",
    "\n",
    "- Recent datasets are so big and complex we're more concerned about underfitting.\n",
    "- Dropout is a much better regularizer.\n",
    "- Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely.\n",
    "\n",
    "**Example:**\n",
    "- Given an input of shape 4x4x5 (HxWxD)\n",
    "- Filter of shape 2x2 (HxW)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "```python\n",
    "new_height = (input_height - filter_height)/S + 1\n",
    "new_width = (input_width - filter_width)/S + 1\n",
    "```\n",
    "\n",
    "**What's the shape of the output?**\n",
    "- new_height = (4 - 2)/2 + 1 = 2\n",
    "- new_width = (4 - 2)/2 + 1 = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shape of pool will be [1, 2, 2, 5], even if padding is changed to 'SAME'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network in TensorFlow\n",
    "\n",
    "- One structure of Convolutional network: A mix of convolutional layers and **max pooling**, followed by fully-connected layers.\n",
    "\n",
    "### Dataset\n",
    "- Import MNIST dataset and using a convenient TensorFlow function to batch, scale, and one-hot encode data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions and Model\n",
    "\n",
    "In TensorFlow, convolution is done using **tf.nn.conv2d()** and **tf.nn.bias_add()** and \n",
    "strides is an array of 4 elements:\n",
    "- First element in this array indicates the stride for batch and last element indicates stride for features. \n",
    "- It's good practice to remove the batches or features to skip from the data set rather than use a stride to skip them. I.e. set the first and last element to 1 in strides in order to use all batches and features.\n",
    "- The middle two elements are the strides for height and width respectively. When someone says they are using a stride of 3, they usually mean `tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])`\n",
    "\n",
    "**3-layer model** uses 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer.\n",
    "\n",
    "Transformation of each layer to new dimensions are shown in the comments.\n",
    "- E.g. the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step.\n",
    "- Then next step applies max pooling, turning each sample into 14x14x32.\n",
    "- All the layers are applied from conv1 to output, producing 10 class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 429 - Loss:   711.9764 Validation Accuracy: 0.952400\n",
      "Epoch  2, Batch 429 - Loss:   371.8660 Validation Accuracy: 0.964400\n",
      "Epoch  3, Batch 429 - Loss:   228.3043 Validation Accuracy: 0.974800\n",
      "Epoch  4, Batch 429 - Loss:   178.6913 Validation Accuracy: 0.975400\n",
      "Epoch  5, Batch 429 - Loss:   144.7359 Validation Accuracy: 0.977800\n",
      "Epoch  6, Batch 429 - Loss:   131.2402 Validation Accuracy: 0.978400\n",
      "Epoch  7, Batch 429 - Loss:   117.3048 Validation Accuracy: 0.979000\n",
      "Epoch  8, Batch 429 - Loss:    94.0407 Validation Accuracy: 0.979600\n",
      "Epoch  9, Batch 429 - Loss:    85.9090 Validation Accuracy: 0.982000\n",
      "Epoch 10, Batch 429 - Loss:    89.1765 Validation Accuracy: 0.981600\n",
      "Testing Accuracy: 0.9794999957084656\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.00001\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "test_valid_size = 256\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "def conv2d(x, W, b, strides=1, padding='SAME'):\n",
    "    y = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
    "    y = tf.nn.bias_add(y, b)\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf. global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        loss = sess.run(cost, feed_dict={x: mnist.validation.images,\n",
    "                                         y: mnist.validation.labels,\n",
    "                                         keep_prob: 1.})\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            x: mnist.validation.images,\n",
    "            y: mnist.validation.labels,\n",
    "            keep_prob: 1.})\n",
    "\n",
    "        print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "            epoch + 1,\n",
    "            batch + 1,\n",
    "            loss,\n",
    "            valid_acc))\n",
    "            \n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images,\n",
    "        y: mnist.test.labels,\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "1. http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "2. https://www.youtube.com/watch?v=ISHGyvsT0QY\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:car2]",
   "language": "python",
   "name": "conda-env-car2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
