{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN \n",
    "\n",
    "### Filters \n",
    "\n",
    "\n",
    "!['Convolutions'](p1.png)\n",
    "\n",
    "- The first step for a CNN is to break up the image into smaller pieces, aka patches. \n",
    "- CNN uses filters to split an image into smaller patches. \n",
    "- The size of these patches matches the filter size.\n",
    "\n",
    "Slide filter horizontally or vertically to focus on a different piece of the image.\n",
    "\n",
    "- The amount by which the filter slides is referred to as the **'stride'**.\n",
    "  - The stride is a hyperparameter which we can tune. \n",
    "- Increasing the stride reduces the size of the model by reducing the number of total patches each layer observes.\n",
    "  - However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "- Important idea: **Grouping together adjacent pixels** and treating them as a collective.\n",
    "\n",
    "    - In a non-convolutional neural network, we would have ignored this adjacency. \n",
    "    - In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have **taken advantage of the fact that pixels in an image are close together for a reason and have special meaning**.\n",
    "\n",
    "    - By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image.\n",
    "\n",
    "##### Filter Depth\n",
    "- It's common to have more than one filter. \n",
    "    - Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. \n",
    "    - The amount of filters in a convolutional layer is called the **filter depth**.\n",
    "    \n",
    "How many neurons does each patch connect to?\n",
    "\n",
    "- If we have a depth of **k**, we connect each patch of pixels to **k** neurons in the next layer.\n",
    "    - This gives us the height of **k** in the next layer, as shown below. \n",
    "    - In practice, **k** is a hyperparameter we tune, and most CNNs tend to pick the same starting values.\n",
    "\n",
    "Having multiple neurons for a given patch ensures that the CNN can learn to capture whatever characteristics from given data. \n",
    "- The CNN isn't \"programmed\" to look for certain characteristics. \n",
    "- Rather, it learns on its own which characteristics to notice.\n",
    "\n",
    "### Tensorflow Strides, Dapth and Padding \n",
    "\n",
    "![](p2.PNG)\n",
    "\n",
    "Given \n",
    "```python\n",
    "\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "\n",
    "# (height, width, input_depth, output_depth) = (8, 8, 3, 20)\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "\n",
    "# (batch, height, width, depth)\n",
    "strides = [1, 2, 2, 1] \n",
    "padding = 'SAME'\n",
    "\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "\n",
    "- Output shape of conv will be [1, 16, 16, 20] - It's 4D to account for batch size.\n",
    "- It's not [1, 14, 14, 20] because padding algorithm TensorFlow uses is not as $ output\\_height = \\frac{(n + 2p -f)}{s} +1  $\n",
    "\n",
    "- If we switch padding from `SAME` to `VALID` then the output shape of [1, 13, 13, 20]\n",
    "\n",
    "Summary: \n",
    "- **SAME Padding**, the output height and width are computed as:\n",
    "    - out_height = ceil( in_height / strides[1] )\n",
    "    - out_width  = ceil( in_width) / strides[2] )\n",
    "    \n",
    "- **VALID Padding**, No padding. Output height and width are computed as:\n",
    "    - $ out\\_height =  \\frac{in\\_height - filter\\_height + 1} {strides[1]} $\n",
    "    - $ out\\_width  =  \\frac{in\\_width  - filter\\_width  + 1} {strides[2]} $\n",
    "\n",
    "- **Non Tensorflow**: $ output\\_height = \\frac{(n + 2p -f)}{s} +1  $ \n",
    "    - n: input height \n",
    "    - p: padding   \n",
    "    - f: filter height\n",
    "    - s: stride \n",
    "\n",
    "### Parameter Sharing\n",
    "\n",
    "The weights, `w`, are shared across patches for a given layer in a CNN to detect the **object or feature** regardless of where in the image the **object** is located.\n",
    "\n",
    "- This is known as *statistical invariance* or *translation invariance*\n",
    "\n",
    "The classification of a given patch in an image is determined by the weights and biases corresponding to that patch.\n",
    "- If we want a **cat** that’s in the top left patch to be classified in the same way as a **cat** in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "- This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. \n",
    "    - Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "- There’s an additional benefit to sharing parameters. \n",
    "    - If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. \n",
    "    - This does not scale well, especially for higher fidelity images. \n",
    "    - Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:car2]",
   "language": "python",
   "name": "conda-env-car2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
