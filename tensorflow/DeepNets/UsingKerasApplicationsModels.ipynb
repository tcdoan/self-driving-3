{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using Keras models without pre-trained weights\n",
    "\n",
    "- So far, we've seen the effectiveness of models pre-trained on ImageNet weights, but what if we specify **weights=None** when we load a model? \n",
    "\n",
    "- Well, we'll instead be randomly initializing the weights, as if we had built a model on our own and were starting from scratch.\n",
    "\n",
    "- There are few situations where this might even be a potential use case - basically, when you have data that is very different from the original data. \n",
    "\n",
    "- However, given the large size of the ImageNet dataset (remember, it's over 14 million images from 1,000 classes!), it's highly unlikely this is really the case - it will almost always make the most sense to start with ImageNet pre-trained weights, and only fine-tune from there.\n",
    "\n",
    "\n",
    "Let's check out what happens when we try to use a pre-made model but set the weights to None \n",
    "- This means no training has occurred yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02892767', 'brassiere', 0.0022033758), ('n03794056', 'mousetrap', 0.0019655433), ('n01734418', 'king_snake', 0.0017350846)]\n"
     ]
    }
   ],
   "source": [
    "# VGG without Pre-trained weights. Set weights=None.\n",
    "from keras.applications.vgg16 import VGG16, decode_predictions\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "\n",
    "image_paths = glob('images/*.jpg')\n",
    "img_path = image_paths[2]\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "plt.imshow(img)\n",
    "\n",
    "### Load VGG16 model, without pre-trained weights. Get wacky predictions.\n",
    "model = VGG16(weights=None)\n",
    "predictions = model.predict(x)\n",
    "print('Predicted:', decode_predictions(predictions, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Transfer Learning\n",
    "\n",
    "- Train a network with ImageNet pre-trained weights as a base\n",
    "    - but with additional network layers of our own added on... \n",
    "    \n",
    "- See the difference between using frozen weights and training on all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet \n",
    "\n",
    "#### Inception architecture \n",
    "\n",
    "```python\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "input_size = 139\n",
    "\n",
    "# Using Inception with ImageNet pre-trained weights\n",
    "inception = InceptionV3(weights=weights_flag, include_top=False,\n",
    "                        input_shape=(input_size,input_size,3))\n",
    "```\n",
    "\n",
    "\n",
    "![Inception architecture](inception.PNG)\n",
    "\n",
    "#### Batch normalization \n",
    "\n",
    "- https://keras.io/api/layers/normalization_layers/batch_normalization\n",
    "- Paper: https://arxiv.org/abs/1502.03167\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained with frozen weights\n",
    "\n",
    "- Frozen weights are often used when only fine-tuning the model. Backpropagation and weight updates will not be applied to any frozen layers during training.\n",
    "\n",
    "- If we have an ImageNet pre-trained model, most of the network is likely applicable to our situation, so we may only need to cut off the top fully-connected layer, freeze all other layers, and just add one or more layers at the end that are not frozen to perform some fine-tuning.\n",
    "\n",
    "- There is also the option of not freezing the weights, which will start  model on the ImageNet pre-trained weights (if applicable) and then perform further training from there.\n",
    "\n",
    "- Freezing the weights also helps \"memory usage\" and \"training speed\" \n",
    "    - for larger networks such as VGG, there is a substantially larger memory and slower speed when it needs to perform backpropagation and weight updates across all layers instead of just on a small portion of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 139, 139, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 69, 69, 32)   864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 69, 69, 32)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 69, 69, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 67, 67, 32)   9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 67, 67, 32)   96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 67, 67, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 67, 67, 64)   18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 67, 67, 64)   192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 67, 67, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 33, 33, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 33, 33, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 33, 33, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 33, 33, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 31, 31, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 31, 31, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 31, 31, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 15, 15, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 15, 15, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 15, 15, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 15, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 15, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 15, 15, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 15, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 15, 15, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 15, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 15, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 15, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 15, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 15, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 15, 15, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 15, 15, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 15, 15, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 15, 15, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 15, 15, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 15, 15, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 15, 15, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 15, 15, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 15, 15, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 15, 15, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 15, 15, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 15, 15, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 15, 15, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 15, 15, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 15, 15, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 15, 15, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 15, 15, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 15, 15, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 15, 15, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 15, 15, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 15, 15, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 15, 15, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 15, 15, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 15, 15, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 15, 15, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 15, 15, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 15, 15, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 15, 15, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 15, 15, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 15, 15, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 15, 15, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 15, 15, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 15, 15, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 15, 15, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 15, 15, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 15, 15, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 15, 15, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 15, 15, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 15, 15, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 15, 15, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 15, 15, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 15, 15, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 15, 15, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 15, 15, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 15, 15, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 15, 15, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 15, 15, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 15, 15, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 15, 15, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 15, 15, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 15, 15, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 15, 15, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 15, 15, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 96)     82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 7, 384)    1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 96)     288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 7, 7, 384)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 96)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 7, 7, 128)    384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 128)    384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 7, 7, 128)    384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 128)    384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 7, 7, 128)    114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 128)    114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 7, 7, 128)    384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 128)    384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 192)    172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 7, 7, 192)    172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 7, 7, 192)    576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 192)    576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 192)    576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 7, 7, 192)    576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 192)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 192)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 160)    480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 160)    480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 160)    480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 160)    480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 160)    179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 160)    179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 160)    480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 7, 7, 160)    480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 160)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 160)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 192)    215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 192)    215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 192)    576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 192)    576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 192)    576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 192)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 192)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 7, 7, 160)    480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 160)    480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 160)    179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 7, 7, 160)    179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 7, 7, 160)    480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 7, 7, 160)    480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 7, 7, 160)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 160)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 192)    215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 7, 7, 192)    215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 192)    576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 7, 7, 192)    576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 7, 7, 192)    576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 7, 7, 192)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 7, 7, 192)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 7, 7, 192)    576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 192)    576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 192)    576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 7, 7, 192)    576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 192)    258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 7, 7, 192)    258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 192)    576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 7, 7, 192)    576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 7, 7, 192)    576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 7, 7, 192)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 7, 7, 192)    576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 7, 7, 192)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 7, 7, 192)    258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 7, 7, 192)    576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 7, 7, 192)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 7, 7, 192)    258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 7, 7, 192)    576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 7, 7, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 7, 7, 192)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 3, 3, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 3, 3, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 3, 3, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 3, 3, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 3, 3, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 3, 3, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 3, 3, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 3, 3, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 3, 3, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 3, 3, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 3, 3, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 3, 3, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 3, 3, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 3, 3, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 3, 3, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 3, 3, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 3, 3, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 3, 3, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 3, 3, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 3, 3, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 3, 3, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 3, 3, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 3, 3, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 3, 3, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 3, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 3, 3, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils import plot_model\n",
    "\n",
    "freeze_flag = True\n",
    "weights_flag = 'imagenet' \n",
    "preprocess_flag = True \n",
    "\n",
    "inception = InceptionV3(weights=weights_flag, include_top=False, input_shape=(139,139,3))\n",
    "# plot_model(inception)\n",
    "\n",
    "# Check out layers of the model.\n",
    "inception.summary()\n",
    "\n",
    "# for idx, layer in enumerate(inception.layers):\n",
    "#     print(\"{:4}: {}\".format(idx, layer)) \n",
    "#     print(inception.layers[-5].activation) \n",
    "  \n",
    "if freeze_flag == True:\n",
    "    for layer in inception.layers:\n",
    "        layer.trainable = False            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping layers\n",
    "\n",
    "#### inception.layers.pop()\n",
    "\n",
    "In a normal Inception network, inception.summary() showed:\n",
    "- The last two layers were a global average pooling layer, and a fully-connected \"Dense\" layer.\n",
    "- However, since $ InceptionV3(..., include\\_top=False) $ is constructed, both last two layers get dropped.\n",
    "\n",
    "To drop additional layers, one would use $model.laypers.pop()$ - which works from the end of the model backwards.\n",
    "\n",
    "```python\n",
    "model.layers.pop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new layers\n",
    "\n",
    "- Keras's Sequential model is for simplicity. Use Model API which functions a little differently. \n",
    "    - Instead of using model.add(), we explicitly tell the model which previous layer to attach to the current layer. \n",
    "    - This is useful for advanced features, e.g. skip layers - which were used heavily in ResNet.\n",
    "    \n",
    "```python\n",
    "\n",
    "## Attach a new dropout layer x, with it's input coming from a layer with the variable name *inp.*\n",
    "x = Dropout(0.2)(inp)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Let's use CIFAR-10 dataset, which consists of 60,000 32x32 images of 10 classes.**\n",
    "- Use Keras's Input function\n",
    "- Re-size the images up to the **input_size** specified earlier (139x139).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 139, 139, 3)       0         \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 22,857,002\n",
      "Trainable params: 1,054,218\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "input_size = 139\n",
    "cifar_input = Input(shape=(32, 32, 3))\n",
    "\n",
    "# Re-sizes the input with Kera's Lambda layer & attach to cifar_input\n",
    "resized_input = Lambda(lambda x: tf.image.resize_images(x, (input_size, input_size)))(cifar_input)\n",
    "\n",
    "# Feeds the re-sized input into Inception model\n",
    "inp = inception(resized_input)\n",
    "\n",
    "## Setting `include_top=False` removed both GlobalAveragePool and Dense layers\n",
    "## Add it back here, and make sure to connect it to the end of Inception\n",
    "x = GlobalAveragePooling2D()(inp)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "predictions = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "# NOW use the actual Model API to create the full model.\n",
    "model = Model(inputs=cifar_input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check the summary of this new model to confirm the architecture\n",
    "# Notice how this method of adding layers before InceptionV3 and appending to the end of it \n",
    "# made InceptionV3 condense down into one line in the summary\n",
    "# If we use the Inception model's normal input (gather from inception.layers.input), it would \n",
    "# instead show all the layers like before.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks\n",
    "\n",
    "Keras callbacks allow you to gather and store additional information during training, such as the best model, or even stop training early if the validation accuracy has stopped improving. These methods can help to avoid overfitting, or avoid other issues.\n",
    "\n",
    "There's two key callbacks to mention here, **ModelCheckpoint** and **EarlyStopping**. As the names may suggest, model checkpoint saves down the best model so far based on a given metric, while early stopping will end training before the specified number of epochs if the chosen metric no longer improves after a given amount of time.\n",
    "\n",
    "**To set these callbacks:**\n",
    "\n",
    "```python\n",
    "checkpoint = ModelCheckpoint(filepath=save_path, monitor='val_loss', save_best_only=True)\n",
    "```\n",
    "\n",
    "This would save a model to a specified $save\\_path$, based on validation loss, and only save down the best models.\n",
    "\n",
    "If set *save_best_only* to False, every single epoch will save down another version of the model.\n",
    "\n",
    "```python\n",
    "stopper = EarlyStopping(monitor='val_acc', min_delta=0.0003, patience=5)\n",
    "```\n",
    "\n",
    "This will monitor validation accuracy, and if it has not decreased by more than 0.0003 from the previous best validation accuracy for 5 epochs, training will end early.\n",
    "\n",
    "Feed these callbacks into $ fit() $ when training the model (along with all other relevant data to feed into fit):\n",
    "\n",
    "\n",
    "```python\n",
    "model.fit(callbacks=[checkpoint, stopper])\n",
    "```\n",
    "\n",
    "#### Check out Keras's ImageDataGenerator docs: https://faroit.com/keras-docs/2.0.9/preprocessing/image\n",
    "\n",
    "- Can also add additional image augmentation through this function \n",
    "- Although we are skipping it from below code. Explore it in the upcoming project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/anaconda3/envs/car2/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/78 [==============================] - 215s 3s/step - loss: 1.5107 - acc: 0.5188 - val_loss: 0.9480 - val_acc: 0.6930\n",
      "Epoch 2/10\n",
      "79/78 [==============================] - 251s 3s/step - loss: 0.9196 - acc: 0.6920 - val_loss: 1.0027 - val_acc: 0.6880\n",
      "Epoch 3/10\n",
      "79/78 [==============================] - 413s 5s/step - loss: 0.8082 - acc: 0.7243 - val_loss: 0.9413 - val_acc: 0.6885\n",
      "Epoch 4/10\n",
      "79/78 [==============================] - 440s 6s/step - loss: 0.7330 - acc: 0.7466 - val_loss: 0.9628 - val_acc: 0.6815\n",
      "Epoch 5/10\n",
      "79/78 [==============================] - 439s 6s/step - loss: 0.6440 - acc: 0.7828 - val_loss: 0.9285 - val_acc: 0.6915\n",
      "Epoch 6/10\n",
      "79/78 [==============================] - 441s 6s/step - loss: 0.5944 - acc: 0.7967 - val_loss: 0.9260 - val_acc: 0.7080\n",
      "Epoch 7/10\n",
      "79/78 [==============================] - 439s 6s/step - loss: 0.5190 - acc: 0.8188 - val_loss: 0.9545 - val_acc: 0.7020\n",
      "Epoch 8/10\n",
      "79/78 [==============================] - 440s 6s/step - loss: 0.4812 - acc: 0.8338 - val_loss: 0.9484 - val_acc: 0.6985\n",
      "Epoch 9/10\n",
      "79/78 [==============================] - 439s 6s/step - loss: 0.4389 - acc: 0.8521 - val_loss: 1.0093 - val_acc: 0.6835\n",
      "Epoch 10/10\n",
      "79/78 [==============================] - 440s 6s/step - loss: 0.4610 - acc: 0.8351 - val_loss: 0.9661 - val_acc: 0.6985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd84848240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_one_hot_train = label_binarizer.fit_transform(y_train)\n",
    "y_one_hot_val = label_binarizer.fit_transform(y_val)\n",
    "\n",
    "X_train, y_one_hot_train = shuffle(X_train, y_one_hot_train)\n",
    "X_val, y_one_hot_val = shuffle(X_val, y_one_hot_val)\n",
    "\n",
    "# Use the first 10,000 images for speed reasons\n",
    "X_train = X_train[:10000]\n",
    "y_one_hot_train = y_one_hot_train[:10000]\n",
    "X_val = X_val[:2000]\n",
    "y_one_hot_val = y_one_hot_val[:2000]\n",
    "\n",
    "# Use a generator to pre-process images for ImageNet\n",
    "if preprocess_flag == True:\n",
    "    datagen     = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "else:\n",
    "    datagen     = ImageDataGenerator()\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "# Note: we aren't using callbacks here since we only are using 5 epochs to conserve GPU time\n",
    "model.fit_generator(datagen.flow(X_train, y_one_hot_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train)/batch_size, epochs=epochs, verbose=1, \n",
    "                    validation_data=val_datagen.flow(X_val, y_one_hot_val, batch_size=batch_size),\n",
    "                    validation_steps=len(X_val)/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "#### Test without frozen weights, or by training from scratch.\n",
    "\n",
    "- IF the majority of the model was frozen above, training speed is pretty quick. \n",
    "- TO check out the training speed, as well as final accuracy without \"freeze the weights\". \n",
    "    - Note that this can be fairly slow, so we're marking this as optional in order to conserve GPU time.\n",
    "    - To see results, go back to the first code cell and set **freeze_flag=False**. \n",
    "    - To completely train from scratch without ImageNet pre-trained weights, set **weights_flag=None**\n",
    "    Then, go to Kernel > Restart & Run All.\n",
    "\n",
    "Training Mode | Val Acc @ 1 epoch | Val Acc @ 5 epoch | Time per epoch\n",
    "---- | :----: | :----: | ----:\n",
    "Frozen weights | 65.5% | 70.3% | 50 seconds\n",
    "Unfrozen weights | 50.6% | 71.6% | 142 seconds\n",
    "No pre-trained weights | 19.2% | 39.2% | 142 seconds\n",
    "\n",
    "From the above, we can see that the pre-trained model with frozen weights actually began converging the fastest (already at 65.5% after 1 epoch), while the model re-training from the pre-trained weights slightly edged it out after 5 epochs.\n",
    "\n",
    "However, this does not tell the whole story - the training accuracy was substantially higher, nearing 87% for the unfrozen weights model. It actually began overfit the data much more under this method. We would likely be able to counteract some of this issue by using data augmentation. On the flip side, the model using frozen weights could also have been improved by actually only freezing a portion of the weights; some of these are likely more specific to ImageNet classes as it gets later in the network, as opposed to the simpler features extracted early in the network.\n",
    "\n",
    "### The Power of Transfer Learning\n",
    "Comparing the last line to the other two really shows the power of transfer learning. After five epochs, a model without ImageNet pre-training had only achieved 39.2% accuracy, compared to over 70% for the other two. As such, pre-training the network has saved substantial time, especially given the additional training time needed when the weights are not frozen.\n",
    "\n",
    "There is also evidence found in various research that pre-training on ImageNet weights will result in a higher overall accuracy than completely training from scratch, even when using a substantially different dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:car2]",
   "language": "python",
   "name": "conda-env-car2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
