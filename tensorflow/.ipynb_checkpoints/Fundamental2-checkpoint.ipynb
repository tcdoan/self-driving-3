{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y = WX + B\n",
    "\n",
    "!['Data shape'](tf2.jpg)\n",
    "\n",
    "For every sample of X (X1, X2, X3), we get logits for label 1 (Y1) and label 2 (Y2).\n",
    "\n",
    "In order to add the bias to the product of WX, we had to turn b into a matrix of the same shape. \n",
    "This is a bit unnecessary, as we can take advantage of an operation called broadcasting used in TensorFlow and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "    mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "3060 (784,)\n",
      "3060 (3,)\n",
      "Loss: 6.138011455535889\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_labels = 3\n",
    "learning_rate = 0.08\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w_dim = (n_features, n_labels)\n",
    "W = tf.Variable(tf.truncated_normal(w_dim))\n",
    "b = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "print(len(train_features), train_features[1].shape)\n",
    "print(len(train_labels), train_labels[1].shape)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    cross_entropy = -tf.reduce_sum(y * tf.log(prediction), reduction_indices=1)\n",
    "    loss = tf.reduce_mean(cross_entropy)    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    _, l = sess.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={X: train_features, y: train_labels})\n",
    "\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n",
      "[[0.09003057 0.00242826 0.01587624 0.33333333]\n",
      " [0.24472847 0.01794253 0.11731043 0.33333333]\n",
      " [0.66524096 0.97962921 0.86681333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    # axis=0 means sum by rows \n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
    "\n",
    "logits = [3.0, 3.0, 3.0]\n",
    "print(softmax(logits))\n",
    "\n",
    "# Each column is a sample (a data point)\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "    \n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batching\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory size\n",
    "\n",
    "```\n",
    "train_features Shape: (55000, 784) Type: float32\n",
    "train_labels Shape: (55000, 10) Type: float32\n",
    "weights Shape: (784, 10) Type: float32\n",
    "bias Shape: (10,) Type: float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172480 Kb\n"
     ]
    }
   ],
   "source": [
    "# How many bytes of memory does train_features need?\n",
    "print(55000 * 784 * 4 // 1000, 'Kb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Mini-batching\n",
    "\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's `tf.placeholder()` function to receive the varying batch sizes.\n",
    "\n",
    "IF each sample had `n_input = 784` features (aka pixels) and `n_classes = 10` possible labels, the dimensions for features would be `[None, n_input]` and labels would be `[None, n_classes]`.\n",
    "\n",
    "```python\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "The **None** dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "\n",
    "```\n",
    "features is (50000, 400)\n",
    "labels is (50000, 10)\n",
    "batch_size is 128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches:  390 , last_batch_size:  80\n"
     ]
    }
   ],
   "source": [
    "num_batches = 50000 // 128\n",
    "last_batch_size = 50000%128\n",
    "print('num_batches: ', num_batches, ', last_batch_size: ', last_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed mini batches of MNIST features and labels into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['F41', 'F42', 'F43', 'F44'],\n",
      "   ['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24']),\n",
      "  (['L41', 'L42'], ['L11', 'L12'], ['L21', 'L22'])],\n",
      " [(['F31', 'F32', 'F33', 'F34'],), (['L31', 'L32'],)]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pprint import pprint\n",
    "def batches(batch_size, features, labels):    \n",
    "    assert len(features) == len(labels)\n",
    "    \n",
    "    \n",
    "    combined = list(zip(features, labels))\n",
    "    np.random.shuffle(combined)\n",
    "    x_train, y_train = zip(*combined)\n",
    "\n",
    "    myBatches = []\n",
    "    N = len(x_train)\n",
    "    k = batch_size    \n",
    "    for i in range(0, N, k):\n",
    "        x = x_train[i:i+k]\n",
    "        y = y_train[i:i+k]\n",
    "        myBatches.append([x, y])\n",
    "        \n",
    "    return myBatches\n",
    "\n",
    "# test\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 1.79     Valid Accuracy: 0.711\n",
      "Epoch: 1    - Cost: 1.32     Valid Accuracy: 0.792\n",
      "Epoch: 2    - Cost: 1.17     Valid Accuracy: 0.823\n",
      "Epoch: 3    - Cost: 1.08     Valid Accuracy: 0.84 \n",
      "Epoch: 4    - Cost: 1.01     Valid Accuracy: 0.851\n",
      "Epoch: 5    - Cost: 0.951    Valid Accuracy: 0.861\n",
      "Epoch: 6    - Cost: 0.899    Valid Accuracy: 0.866\n",
      "Epoch: 7    - Cost: 0.853    Valid Accuracy: 0.872\n",
      "Epoch: 8    - Cost: 0.812    Valid Accuracy: 0.876\n",
      "Epoch: 9    - Cost: 0.776    Valid Accuracy: 0.879\n",
      "Epoch: 10   - Cost: 0.744    Valid Accuracy: 0.88 \n",
      "Epoch: 11   - Cost: 0.716    Valid Accuracy: 0.883\n",
      "Epoch: 12   - Cost: 0.691    Valid Accuracy: 0.885\n",
      "Epoch: 13   - Cost: 0.668    Valid Accuracy: 0.888\n",
      "Epoch: 14   - Cost: 0.648    Valid Accuracy: 0.889\n",
      "Epoch: 15   - Cost: 0.63     Valid Accuracy: 0.891\n",
      "Epoch: 16   - Cost: 0.613    Valid Accuracy: 0.893\n",
      "Epoch: 17   - Cost: 0.598    Valid Accuracy: 0.894\n",
      "Epoch: 18   - Cost: 0.585    Valid Accuracy: 0.895\n",
      "Epoch: 19   - Cost: 0.573    Valid Accuracy: 0.894\n",
      "Epoch: 20   - Cost: 0.562    Valid Accuracy: 0.896\n",
      "Epoch: 21   - Cost: 0.552    Valid Accuracy: 0.898\n",
      "Epoch: 22   - Cost: 0.542    Valid Accuracy: 0.899\n",
      "Epoch: 23   - Cost: 0.534    Valid Accuracy: 0.901\n",
      "Epoch: 24   - Cost: 0.527    Valid Accuracy: 0.902\n",
      "Epoch: 25   - Cost: 0.52     Valid Accuracy: 0.901\n",
      "Epoch: 26   - Cost: 0.513    Valid Accuracy: 0.903\n",
      "Epoch: 27   - Cost: 0.507    Valid Accuracy: 0.903\n",
      "Epoch: 28   - Cost: 0.502    Valid Accuracy: 0.904\n",
      "Epoch: 29   - Cost: 0.497    Valid Accuracy: 0.904\n",
      "Epoch: 30   - Cost: 0.492    Valid Accuracy: 0.905\n",
      "Epoch: 31   - Cost: 0.488    Valid Accuracy: 0.907\n",
      "Epoch: 32   - Cost: 0.484    Valid Accuracy: 0.906\n",
      "Epoch: 33   - Cost: 0.48     Valid Accuracy: 0.907\n",
      "Epoch: 34   - Cost: 0.477    Valid Accuracy: 0.908\n",
      "Epoch: 35   - Cost: 0.474    Valid Accuracy: 0.908\n",
      "Epoch: 36   - Cost: 0.47     Valid Accuracy: 0.909\n",
      "Epoch: 37   - Cost: 0.467    Valid Accuracy: 0.908\n",
      "Epoch: 38   - Cost: 0.465    Valid Accuracy: 0.909\n",
      "Epoch: 39   - Cost: 0.462    Valid Accuracy: 0.909\n",
      "Epoch: 40   - Cost: 0.459    Valid Accuracy: 0.909\n",
      "Epoch: 41   - Cost: 0.457    Valid Accuracy: 0.91 \n",
      "Epoch: 42   - Cost: 0.455    Valid Accuracy: 0.91 \n",
      "Epoch: 43   - Cost: 0.452    Valid Accuracy: 0.91 \n",
      "Epoch: 44   - Cost: 0.45     Valid Accuracy: 0.911\n",
      "Epoch: 45   - Cost: 0.448    Valid Accuracy: 0.911\n",
      "Epoch: 46   - Cost: 0.446    Valid Accuracy: 0.911\n",
      "Epoch: 47   - Cost: 0.444    Valid Accuracy: 0.911\n",
      "Epoch: 48   - Cost: 0.442    Valid Accuracy: 0.911\n",
      "Epoch: 49   - Cost: 0.44     Valid Accuracy: 0.911\n",
      "Epoch: 50   - Cost: 0.439    Valid Accuracy: 0.911\n",
      "Epoch: 51   - Cost: 0.437    Valid Accuracy: 0.911\n",
      "Epoch: 52   - Cost: 0.435    Valid Accuracy: 0.911\n",
      "Epoch: 53   - Cost: 0.433    Valid Accuracy: 0.912\n",
      "Epoch: 54   - Cost: 0.432    Valid Accuracy: 0.913\n",
      "Epoch: 55   - Cost: 0.43     Valid Accuracy: 0.913\n",
      "Epoch: 56   - Cost: 0.429    Valid Accuracy: 0.913\n",
      "Epoch: 57   - Cost: 0.427    Valid Accuracy: 0.913\n",
      "Epoch: 58   - Cost: 0.426    Valid Accuracy: 0.913\n",
      "Epoch: 59   - Cost: 0.424    Valid Accuracy: 0.912\n",
      "Epoch: 60   - Cost: 0.423    Valid Accuracy: 0.912\n",
      "Epoch: 61   - Cost: 0.421    Valid Accuracy: 0.912\n",
      "Epoch: 62   - Cost: 0.42     Valid Accuracy: 0.913\n",
      "Epoch: 63   - Cost: 0.418    Valid Accuracy: 0.913\n",
      "Epoch: 64   - Cost: 0.417    Valid Accuracy: 0.914\n",
      "Epoch: 65   - Cost: 0.416    Valid Accuracy: 0.913\n",
      "Epoch: 66   - Cost: 0.414    Valid Accuracy: 0.913\n",
      "Epoch: 67   - Cost: 0.413    Valid Accuracy: 0.914\n",
      "Epoch: 68   - Cost: 0.412    Valid Accuracy: 0.913\n",
      "Epoch: 69   - Cost: 0.41     Valid Accuracy: 0.913\n",
      "Epoch: 70   - Cost: 0.409    Valid Accuracy: 0.913\n",
      "Epoch: 71   - Cost: 0.408    Valid Accuracy: 0.913\n",
      "Epoch: 72   - Cost: 0.407    Valid Accuracy: 0.914\n",
      "Epoch: 73   - Cost: 0.405    Valid Accuracy: 0.914\n",
      "Epoch: 74   - Cost: 0.404    Valid Accuracy: 0.914\n",
      "Epoch: 75   - Cost: 0.403    Valid Accuracy: 0.914\n",
      "Epoch: 76   - Cost: 0.402    Valid Accuracy: 0.914\n",
      "Epoch: 77   - Cost: 0.4      Valid Accuracy: 0.914\n",
      "Epoch: 78   - Cost: 0.399    Valid Accuracy: 0.914\n",
      "Epoch: 79   - Cost: 0.398    Valid Accuracy: 0.914\n",
      "Epoch: 80   - Cost: 0.397    Valid Accuracy: 0.914\n",
      "Epoch: 81   - Cost: 0.396    Valid Accuracy: 0.914\n",
      "Epoch: 82   - Cost: 0.395    Valid Accuracy: 0.915\n",
      "Epoch: 83   - Cost: 0.394    Valid Accuracy: 0.916\n",
      "Epoch: 84   - Cost: 0.392    Valid Accuracy: 0.916\n",
      "Epoch: 85   - Cost: 0.391    Valid Accuracy: 0.916\n",
      "Epoch: 86   - Cost: 0.39     Valid Accuracy: 0.916\n",
      "Epoch: 87   - Cost: 0.389    Valid Accuracy: 0.917\n",
      "Epoch: 88   - Cost: 0.388    Valid Accuracy: 0.917\n",
      "Epoch: 89   - Cost: 0.387    Valid Accuracy: 0.917\n",
      "Epoch: 90   - Cost: 0.386    Valid Accuracy: 0.917\n",
      "Epoch: 91   - Cost: 0.385    Valid Accuracy: 0.917\n",
      "Epoch: 92   - Cost: 0.384    Valid Accuracy: 0.917\n",
      "Epoch: 93   - Cost: 0.383    Valid Accuracy: 0.917\n",
      "Epoch: 94   - Cost: 0.382    Valid Accuracy: 0.917\n",
      "Epoch: 95   - Cost: 0.381    Valid Accuracy: 0.918\n",
      "Epoch: 96   - Cost: 0.38     Valid Accuracy: 0.918\n",
      "Epoch: 97   - Cost: 0.379    Valid Accuracy: 0.918\n",
      "Epoch: 98   - Cost: 0.378    Valid Accuracy: 0.918\n",
      "Epoch: 99   - Cost: 0.377    Valid Accuracy: 0.918\n",
      "Test Accuracy: 0.9129999876022339\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={x: last_features, y: last_labels})\n",
    "    \n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: valid_features, y: valid_labels})\n",
    "    \n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "    \n",
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "train_features = mnist.train.images\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "\n",
    "valid_features = mnist.validation.images\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "\n",
    "test_features  = mnist.test.images\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Logits - xW + b or (128*784) * (784*10) + (,10) \n",
    "z = tf.add(tf.matmul(x, W), b)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(z, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch_i in range(epochs):        \n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                    x: batch_features,\n",
    "                    y: batch_labels,\n",
    "                    learning_rate: learn_rate}\n",
    "            sess.run(opt, feed_dict=train_feed_dict)\n",
    "        \n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "        # learn_rate = learn_rate*(100-epoch_i)/100.0\n",
    "        \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: test_features, y: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
